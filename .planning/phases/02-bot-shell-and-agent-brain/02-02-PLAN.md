---
phase: 02-bot-shell-and-agent-brain
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - src/memory/short-term.ts
  - src/memory/medium-term.ts
  - src/memory/long-term.ts
  - src/memory/embedder.ts
  - src/memory/types.ts
autonomous: true
requirements: [MSG-04]

must_haves:
  truths:
    - "Short-term memory stores and retrieves today's messages from Redis with 24h TTL"
    - "Medium-term memory stores messages in PostgreSQL and retrieves last N days of conversation"
    - "Long-term memory embeds messages into Qdrant and returns semantically similar past conversations"
    - "Embedding pipeline generates vectors locally without external API keys"
  artifacts:
    - path: "src/memory/short-term.ts"
      provides: "Redis-backed short-term message storage"
      exports: ["ShortTermMemory"]
    - path: "src/memory/medium-term.ts"
      provides: "PostgreSQL-backed medium-term message storage and query"
      exports: ["MediumTermMemory"]
    - path: "src/memory/long-term.ts"
      provides: "Qdrant-backed semantic search over all messages"
      exports: ["LongTermMemory"]
    - path: "src/memory/embedder.ts"
      provides: "Local ONNX embedding pipeline for multilingual text"
      exports: ["embed", "initEmbedder"]
    - path: "src/memory/types.ts"
      provides: "Shared memory types (StoredMessage, SearchResult)"
      exports: ["StoredMessage", "SearchResult"]
  key_links:
    - from: "src/memory/long-term.ts"
      to: "src/memory/embedder.ts"
      via: "embed() called for vector generation"
      pattern: "embed\\("
    - from: "src/memory/medium-term.ts"
      to: "src/db/schema.ts"
      via: "Drizzle queries on messages table"
      pattern: "messages"
    - from: "src/memory/short-term.ts"
      to: "ioredis"
      via: "Redis list operations for message storage"
      pattern: "redis\\."
---

<objective>
Build the three-tier memory system: Redis for short-term (today's conversations), PostgreSQL for medium-term (~7 days of context), and Qdrant for long-term semantic search (all messages, all time). This is the core of conversation context retention.

Purpose: MSG-04 requires multi-step conversations with context retention. The three-tier memory model (from locked user decisions) feeds conversation history into Claude's prompt, enabling "you mentioned X this morning" and semantic recall of past discussions. Each tier has different retention, speed, and query characteristics.

Output: Five files implementing the complete memory subsystem with store/retrieve operations for each tier.
</objective>

<execution_context>
@C:/Users/dimsh/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/dimsh/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-bot-shell-and-agent-brain/02-RESEARCH.md
@src/db/schema.ts
@src/db/index.ts
@src/config/env.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement shared types, short-term memory (Redis), and medium-term memory (PostgreSQL)</name>
  <files>src/memory/types.ts, src/memory/short-term.ts, src/memory/medium-term.ts</files>
  <action>
Create `src/memory/types.ts` with shared types:

```typescript
export interface StoredMessage {
  id: string
  channelType: 'telegram' | 'slack'
  channelId: string
  userId: string
  role: 'user' | 'assistant'
  text: string
  language?: 'ru' | 'en'
  timestamp: Date
  metadata?: Record<string, unknown>
}

export interface SearchResult {
  message: StoredMessage
  score: number  // relevance score (0-1 for semantic, exact for keyword)
}
```

Create `src/memory/short-term.ts` — Redis-backed store for today's messages:
- Class `ShortTermMemory` with constructor accepting `Redis` instance (from ioredis)
- `store(channelId: string, message: StoredMessage): Promise<void>` — LPUSH to `chat:{channelId}:messages`, LTRIM to keep last 100, EXPIRE with 86400 seconds (24h TTL)
- `getRecent(channelId: string, count: number): Promise<StoredMessage[]>` — LRANGE from 0 to count-1, parse JSON, return array
- `clear(channelId: string): Promise<void>` — DEL the key
- All messages serialized/deserialized with JSON.stringify/parse
- Date fields must be handled: store as ISO string, reconstruct as Date on retrieval

Create `src/memory/medium-term.ts` — PostgreSQL-backed store:
- Class `MediumTermMemory` with constructor accepting drizzle database instance (the `db` from `src/db/index.ts`)
- `store(message: StoredMessage): Promise<void>` — Insert into `messages` table using Drizzle ORM
- `getRecent(channelId: string, days: number, limit: number): Promise<StoredMessage[]>` — Query messages where channelId matches AND createdAt >= (now - days), order by createdAt DESC, limit rows, map to StoredMessage
- `getByDateRange(channelId: string, from: Date, to: Date, limit: number): Promise<StoredMessage[]>` — Query messages in date range
- `search(channelId: string, keyword: string, limit: number): Promise<StoredMessage[]>` — Use Drizzle `ilike` for simple text search on the `text` column (PostgreSQL full-text search can be added later)

Import `db` from `../db/index.js`, import `messages` from `../db/schema.js`. Use `eq`, `and`, `gte`, `desc`, `ilike` from `drizzle-orm`. Follow ESM import conventions with `.js` extensions.
  </action>
  <verify>
    <automated>npx tsc --noEmit</automated>
    <manual>Verify ShortTermMemory and MediumTermMemory classes export correctly with all methods</manual>
  </verify>
  <done>Short-term memory stores messages in Redis with 24h TTL and retrieves recent messages. Medium-term memory persists messages to PostgreSQL with date range and keyword queries. Both compile cleanly.</done>
</task>

<task type="auto">
  <name>Task 2: Implement embedding pipeline and long-term memory (Qdrant semantic search)</name>
  <files>src/memory/embedder.ts, src/memory/long-term.ts</files>
  <action>
First, install the embedding library:
```bash
npm install @huggingface/transformers
```

Create `src/memory/embedder.ts` — local ONNX embedding pipeline:
- Use `import { pipeline } from '@huggingface/transformers'`
- Module-level variable: `let embedder: Awaited<ReturnType<typeof pipeline>> | null = null`
- `initEmbedder(): Promise<void>` — Initialize the pipeline with model `Xenova/paraphrase-multilingual-MiniLM-L12-v2`, task `feature-extraction`, dtype `fp32`. Store in module variable. Log initialization start and completion. This should be called during startup to trigger model download.
- `embed(text: string): Promise<number[]>` — Call embedder with `{ pooling: 'mean', normalize: true }`, extract Float32Array from output.data, return as number[]. Throw if embedder not initialized.
- `getEmbeddingDimension(): number` — Return 384 (the dimension of MiniLM-L12-v2)
- Add logging with the project's pino logger for init progress

Create `src/memory/long-term.ts` — Qdrant-backed semantic search:
- Class `LongTermMemory` with constructor accepting `QdrantClient` instance
- Constant `COLLECTION_NAME = 'astra_messages'`
- Constant `VECTOR_SIZE = 384`
- `ensureCollection(): Promise<void>` — Check if collection exists via `getCollections()`, create if not with Cosine distance and VECTOR_SIZE dimensions. Create payload indexes on `channel_type` (keyword), `channel_id` (keyword), `timestamp` (integer). Idempotent — safe to call multiple times.
- `store(message: StoredMessage, vector: number[]): Promise<void>` — Upsert point with UUID as point ID (use `crypto.randomUUID()`), vector, and payload containing: channel_type, channel_id, user_id, role, text, language, timestamp (as epoch ms), message_id (the StoredMessage.id)
- `search(query: string, limit: number, channelId?: string): Promise<SearchResult[]>` — Embed the query text via `embed()`, search Qdrant with optional channel_id filter, return results mapped to SearchResult[] with score
- `searchByVector(vector: number[], limit: number, channelId?: string): Promise<SearchResult[]>` — Direct vector search (for when embedding is pre-computed)

Import QdrantClient from `@qdrant/js-client-rest`. Import `embed` from `./embedder.js`. Import `StoredMessage`, `SearchResult` from `./types.js`. Use `crypto.randomUUID()` for point IDs.

IMPORTANT: The embedding call in `search()` is synchronous with the search. For `store()`, the caller (context builder) will handle async embedding to avoid blocking the response path.
  </action>
  <verify>
    <automated>npx tsc --noEmit</automated>
    <manual>Verify LongTermMemory class exports with ensureCollection, store, search methods; embedder exports initEmbedder and embed</manual>
  </verify>
  <done>Embedding pipeline initializes a local multilingual ONNX model (384 dimensions, no API key). Long-term memory stores message vectors in Qdrant and retrieves semantically similar messages. TypeScript compiles cleanly.</done>
</task>

</tasks>

<verification>
- `npx tsc --noEmit` passes with zero errors
- `src/memory/types.ts` exports StoredMessage and SearchResult
- `src/memory/short-term.ts` exports ShortTermMemory with store/getRecent/clear
- `src/memory/medium-term.ts` exports MediumTermMemory with store/getRecent/getByDateRange/search
- `src/memory/embedder.ts` exports initEmbedder, embed, getEmbeddingDimension
- `src/memory/long-term.ts` exports LongTermMemory with ensureCollection/store/search/searchByVector
- Package.json includes @huggingface/transformers dependency
</verification>

<success_criteria>
- Three-tier memory system fully typed and implemented
- Short-term: Redis with 24h TTL, recent message retrieval
- Medium-term: PostgreSQL with date-range queries and keyword search
- Long-term: Qdrant with semantic search using local multilingual embeddings
- All modules compile cleanly, no external API keys required for embeddings
</success_criteria>

<output>
After completion, create `.planning/phases/02-bot-shell-and-agent-brain/02-02-SUMMARY.md`
</output>
